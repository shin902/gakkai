{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),  # padding='same' に変更\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding='same'),  # padding='same' に変更\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder1 = DoubleConv(3, 64)\n",
    "        self.encoder2 = DoubleConv(64, 128)\n",
    "        self.encoder3 = DoubleConv(128, 256)\n",
    "        self.encoder4 = DoubleConv(256, 512)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder4 = DoubleConv(1024, 512)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = DoubleConv(512, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = DoubleConv(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = DoubleConv(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 入力サイズを保存\n",
    "        input_size = x.shape[2:]\n",
    "\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        p1 = F.max_pool2d(enc1, 2)\n",
    "\n",
    "        enc2 = self.encoder2(p1)\n",
    "        p2 = F.max_pool2d(enc2, 2)\n",
    "\n",
    "        enc3 = self.encoder3(p2)\n",
    "        p3 = F.max_pool2d(enc3, 2)\n",
    "\n",
    "        enc4 = self.encoder4(p3)\n",
    "        p4 = F.max_pool2d(enc4, 2)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(p4)\n",
    "\n",
    "        # Decoder with size matching\n",
    "        d4 = self.upconv4(bottleneck)\n",
    "        d4 = torch.cat([d4, F.interpolate(enc4, size=d4.shape[2:])], dim=1)\n",
    "        d4 = self.decoder4(d4)\n",
    "\n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = torch.cat([d3, F.interpolate(enc3, size=d3.shape[2:])], dim=1)\n",
    "        d3 = self.decoder3(d3)\n",
    "\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat([d2, F.interpolate(enc2, size=d2.shape[2:])], dim=1)\n",
    "        d2 = self.decoder2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat([d1, F.interpolate(enc1, size=d1.shape[2:])], dim=1)\n",
    "        d1 = self.decoder1(d1)\n",
    "\n",
    "        # 最終出力を入力サイズにリサイズ\n",
    "        output = self.final_conv(d1)\n",
    "        output = F.interpolate(output, size=input_size, mode='bilinear', align_corners=True)\n",
    "        \"\"\"\n",
    "        print(f\"Input size: {x.shape}\")\n",
    "        print(f\"Encoder1 output: {enc1.shape}\")\n",
    "        print(f\"Decoder1 output: {d1.shape}\")\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.sigmoid(output)\n",
    "# Dataset for noisy image pairs\n",
    "\n",
    "\n",
    "class NoisyDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        # 基本的な変換を定義（ToTensorとNormalize）\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5],  # RGB各チャンネルの平均\n",
    "                                 std=[0.5, 0.5, 0.5])     # RGB各チャンネルの標準偏差\n",
    "        ])\n",
    "        self.custom_transform = transform  # 追加の変換が必要な場合用\n",
    "        self.image_pairs = []\n",
    "\n",
    "        # Find all image pairs\n",
    "        if not self.root_dir.exists():\n",
    "            raise FileNotFoundError(f\"Directory {root_dir} not found\")\n",
    "\n",
    "        # Initialize image pairs\n",
    "        for folder in self.root_dir.iterdir():\n",
    "            if folder.is_dir():\n",
    "                noisy_images = list(folder.glob('*.jpg'))\n",
    "                if len(noisy_images) >= 2:\n",
    "                    self.image_pairs.append((noisy_images[0], noisy_images[1]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_pairs):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "        img1_path, img2_path = self.image_pairs[idx]\n",
    "\n",
    "        # Load and convert images\n",
    "        input_img = Image.open(str(img1_path)).convert('RGB')\n",
    "        target_img = Image.open(str(img2_path)).convert('RGB')\n",
    "\n",
    "        # Resize images\n",
    "        size = (780, 972)\n",
    "        input_img = input_img.resize(size, Image.BILINEAR)\n",
    "        target_img = target_img.resize(size, Image.BILINEAR)\n",
    "\n",
    "        # 基本的な変換を適用（ToTensorとNormalize）\n",
    "        input_tensor = self.transform(input_img)\n",
    "        target_tensor = self.transform(target_img)\n",
    "\n",
    "        # 追加の変換がある場合は適用\n",
    "        if self.custom_transform:\n",
    "            input_tensor = self.custom_transform(input_tensor)\n",
    "            target_tensor = self.custom_transform(target_tensor)\n",
    "\n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "# Training class\n",
    "class Noise2Noise:\n",
    "    def __init__(self, train_dir, valid_dir, model_dir, device):\n",
    "        self.device = device\n",
    "        self.model_dir = Path(model_dir)\n",
    "        self.model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        self.transform = transforms.ToTensor()  # 必要なら他の変換も追加\n",
    "        # Initialize model\n",
    "        self.model = UNet().to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # Setup datasets with additional transforms if needed\n",
    "        additional_transforms = None  # 必要に応じて追加の transforms を定義\n",
    "\n",
    "        try:\n",
    "            self.train_dataset = NoisyDataset(train_dir, additional_transforms)\n",
    "            self.valid_dataset = NoisyDataset(valid_dir, additional_transforms)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error initializing datasets: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Setup dataloaders with smaller batch size\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=8,\n",
    "            shuffle=True,\n",
    "            num_workers=0  # MacのMPSデバイスを使用する場合は0を推奨\n",
    "        )\n",
    "        self.valid_loader = DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=8,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "    def train(self, epochs):\n",
    "        best_valid_loss = float('inf')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "\n",
    "            for batch_idx, (input_img, target_img) in enumerate(self.train_loader):\n",
    "                input_img = input_img.to(self.device)\n",
    "                target_img = target_img.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(input_img)\n",
    "                loss = self.criterion(output, target_img)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f'Epoch {epoch+1}/{epochs} [{batch_idx}/{len(self.train_loader)}] '\n",
    "                            f'Loss: {loss.item():.6f}')\n",
    "\n",
    "            # Validation\n",
    "            valid_loss = self.validate()\n",
    "            print(f'Epoch {epoch+1} Average Train Loss: {train_loss/len(self.train_loader):.6f} '\n",
    "                    f'Valid Loss: {valid_loss:.6f}')\n",
    "\n",
    "            # Save best model\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                self.save_model('test_model.pth')\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        valid_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for input_img, target_img in self.valid_loader:\n",
    "                input_img = input_img.to(self.device)\n",
    "                target_img = target_img.to(self.device)\n",
    "\n",
    "                output = self.model(input_img)\n",
    "                loss = self.criterion(output, target_img)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        return valid_loss / len(self.valid_loader)\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        save_path = self.model_dir / filename\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, save_path)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        load_path = self.model_dir / filename\n",
    "        checkpoint = torch.load(load_path, weights_only=True)  # weights_only=True を追加\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    def denoise_image(self, input_path, output_path):\n",
    "        self.model.eval()\n",
    "\n",
    "        # 画像の読み込みと変換\n",
    "        input_img = Image.open(input_path).convert('RGB')\n",
    "        input_tensor = self.transform(input_img).unsqueeze(0).to(self.device)\n",
    "\n",
    "        # モデルの推論\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "\n",
    "        # 出力テンソルを入力画像サイズにリサイズ\n",
    "        output = F.interpolate(output, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # テンソルを画像に変換して保存\n",
    "        output_img = transforms.ToPILImage()(output.squeeze(0).cpu())\n",
    "        output_img.save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup device\n",
    "    device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = Noise2Noise(\n",
    "        train_dir=\"../../Resources/AI/train_data\",\n",
    "        valid_dir=\"../../Resources/AI/valid_data\",\n",
    "        model_dir=\"../../Resources/AI/model_dir\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train(epochs=1000)\n",
    "    # trainer.load_model('best_model.pth')\n",
    "\n",
    "    # Denoise a single image\n",
    "    trainer.denoise_image(\"../../Resources/Images/19_57_44/001.jpg\", \"../../Resources/Input and Output/output/001-19_57_44.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
